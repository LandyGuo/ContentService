#coding=utf8
import urllib, json, re, logging
from datetime import datetime
from contentservice.crawler import Crawler, Scheduler, Priority, export
from contentservice.models.video import VideoSourceModel, VideoItemModel
from contentservice.utils.datetimeutil import parse_date

logger = logging.getLogger('contentservice')

'''
浠缃椤电json涓峰瑙棰淇℃
'''

LIST = "http://jsonfe.funshion.com/list/?cli=aphone&ver=0.0.0.1&ta=0&type=%s&pagesize=24\
      &page=%s&cate=all&region=all&rdate=all&karma=all&udate=all&order=pl"
      
DETAIL = "http://jsonfe.funshion.com/media/?cli=aphone&ver=0.0.0.1&ta=0&mid=%s"

MOVIE_PLAY = "http://m.funshion.com/play?mediaid=%s"

PLAY_URL = "http://m.funshion.com/play?mediaid=%s&number=%s"

SOURCE = 'fengxing'

CHANNELS = [u'靛奖',u'佃17',u'缁艰',u'ㄦ极']

DICT={
        u"靛奖": "movie",
        u"佃17": "tv",
        u"缁艰": "variety",
        u"ㄦ极": "cartoon"
        }

class ListCrawler(Crawler):
    
    type = 'video.fengxing.list'
    @staticmethod
    def init(conf = None):
        if not conf:
            conf = {}
        Scheduler.schedule(
                           ListCrawler.type, #绫诲
                           key = "", #璇ョ被涓涓瀹渚17,key璁17""
                           priority = conf.get('priority', Priority.High), #浼绾т负楂17
                           data = {'updated' : datetime.min}, #版涓轰娆＄扮拌棰存版堕
                           interval = conf.get('interval', 3600) #寰撮涓17
                           )
    def crawl(self):
        for channel in CHANNELS:            
            list_url=LIST % (DICT.get(channel), 1)#杩ユ绗涓椤17
            pagenum=int(loadurl(list_url).get('pagenum'))
            for page in range(pagenum):
                page+=1#褰椤典1璁℃
                current_url=LIST % (DICT.get(channel), page)
                lists=loadurl(current_url).get('lists')
                for episode in lists:
                    data={
                            'title':episode.get('name'),
                            'image':episode.get('pic'),
                            'category':episode.get('cate'),
                            'channel':channel,
                            'source':SOURCE
                        }
                    Scheduler.schedule(
                                       AlbumCrawler.type,
                                       episode.get('mid'),
                                       data,
                                       reset=True
                                       )

class AlbumCrawler(Crawler):
    type='video.fengxing.album'
     
    
    #琚瑙ｆurl瑙锛17
    '''
    归煎锛17
    璇椤碉http://m.funshion.com/subject?mediaid=104723
    鹃〉锛http://m.funshion.com/play?mediaid=1283             #靛奖
          http://m.funshion.com/play?mediaid=104723&number=1  #跺
    '''
    @staticmethod
    def extract_key(url):
        url=url.strip()
        pram=re.findall('^http://m.funshion.com/\w+\?mediaid=(\d+)',url)
        return pram[0] if pram else None
             
    def crawl(self):
        videos = []
        mid = self.key
        url = DETAIL % mid
        detail = loadurl(url)
        description = detail.get('plots')
        description = ''.join(description.split())
        if self.data.get('channel') == u'靛奖':
            dict_ = detail['pinfos']['mpurls']
            video = VideoItemModel({
                                    "title": self.data.get('title'),
                                    "url": MOVIE_PLAY % mid, #缃椤靛板
                                    "image": self.data.get('image'),
                                    "description": description,
                                    "stream": [{
                                                 'url': dict_['tv'].get('url'),
                                                 'size': dict_['tv'].get('bits'),
                                                 'format': 'mp4'
                                                }]
                                    })   
            videos.append(video)
        else:
            try:
                sort = detail['pinfos'].get('sort')[0]    
                episodes = detail['pinfos']['content'][sort]['fsps']
            except:
                episodes = detail['pinfos']['fsps']

            for episode in episodes:
                plots = episode.get('plots')
                plots = ''.join(plots.split())                
                video = VideoItemModel({
                                     "title": episode.get('taskname'),
                                     "url": PLAY_URL % (mid,episode.get('number')), #缃椤靛板
                                     "image": episode.get('picurl'),
                                     "description": plots,
                                     "stream": getstream(episode.get('mpurls'))
                                     })
                videos.append(video)           
        model = VideoSourceModel({
                                 "source": self.data.get('source'), 
                                 "source_id": mid, #婧绔ID
                                 "title": self.data["title"],
                                 "url": detail.get('shareurl'), #璇椤电板
                                 "image": self.data.get('image'), #剧url
                                 "categories": self.data.get('category'), #绫
                                 "channel": self.data.get('channel'), #棰
                                 "region": detail.get('country'), #板
                                 "videos": videos, #瑙棰涓杈
                                 "pubtime": parse_date(detail.get('rinfo').split(' ')[0]), #涓堕
                                 "actors": detail.get('lactor'),
                                 "directors": detail.get('director'),
                                 "description": description,
                                 })
        #瀵煎烘版
        export(model)
        self.data['to_album_id'] = model['to_album_id']
              
                
                
def loadurl(url):
    page = urllib.urlopen(url)
    data = page.read()
    data = json.loads(data).get('data')
    return  data
                     
def getstream(dic):
    d = {}
    stream = []
    for v in dic.values():
        d['url'] = v.get('url')
        d['size'] = v.get('bits')
        d['format'] = 'mp4'
        stream.append(d)
    return stream
 
if __name__=='__main__':
    #test
    ListCrawler(data = {'updated' : datetime.min}).crawl()  
    #test
    '''
    data={
          'title':"榧宸17",
          'image':"http://img.funshion.com/pictures/198/495/3/1984953_s.jpg",
          'category':['绫诲',' 绗', '','浜插'],
          'channel':'ㄦ极'
         }
         
    AlbumCrawler(key="95356",data=data).crawl()
    '''